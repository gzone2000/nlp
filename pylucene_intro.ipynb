{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "pylucene_intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgpark88/nlp/blob/main/pylucene_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHKzMESIVYgV"
      },
      "source": [
        "참조사이트 : https://notebook.community/paulovn/ml-vm-notebook/vmfiles/IPNB/Examples/g%20Misc/20%20Pylucene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pitq70cVYgW"
      },
      "source": [
        "# Initialization\n",
        "Importing lucene will bring into the Python context all the lucene namespace; from then all lucene modules can be imported (included the support Java modules)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkqC82G8VYgW"
      },
      "source": [
        "import lucene"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KEyPmYRVYgZ",
        "outputId": "17bbb5f0-df03-42a8-de73-9874b55c9b11"
      },
      "source": [
        "print(lucene.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wG7gtVhmVYgc",
        "outputId": "bb817a09-939c-4bd9-aa1c-62c0447753da"
      },
      "source": [
        "# We can check all the Lucene packages included in this distribution of Pylucene\n",
        "for p in sorted(lucene.CLASSPATH.split(':')):\n",
        "    print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\antlr4-runtime-4.5.1-1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\asm-7.2.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\asm-commons-7.2.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\extensions.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\hppc-0.8.1.jar\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-analyzers-common-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-analyzers-kuromoji-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-analyzers-nori-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-analyzers-stempel-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-backward-codecs-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-classification-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-codecs-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-core-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-expressions-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-facet-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-grouping-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-highlighter-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-join-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-memory-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-misc-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-queries-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-queryparser-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-sandbox-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-spatial3d-8.6.1.jar;c\n",
            "\\python37\\lib\\site-packages\\lucene-8.6.1-py3.7-win-amd64.egg\\lucene\\lucene-suggest-8.6.1.jar;c\n",
            "c\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Nz7QASVYge"
      },
      "source": [
        "### The first operation is always to initialize the lucene backend.  This only needs to be done once for each running Python process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S8aJhN0VYge"
      },
      "source": [
        "if not lucene.getVMEnv():\n",
        "    lucene.initVM(vmargs=['-Djava.awt.headless=true'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7laUPKcVYgg"
      },
      "source": [
        "# Tests\n",
        "Let's test a few Lucene components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VmM_S7SVYgh"
      },
      "source": [
        "test_strings = (\n",
        "    \"PyLucene is a Python extension for accessing Java Lucene.\",\n",
        "    \"Its goal is to allow you to use Lucene's text indexing and searching capabilities from Python.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaqUq7jRVYgj"
      },
      "source": [
        "from org.apache.lucene.analysis.tokenattributes import CharTermAttribute\n",
        "\n",
        "def fetch_terms(obj):\n",
        "    '''fetch all terms from a token list object, as strings'''\n",
        "    termAtt = obj.getAttribute(CharTermAttribute.class_)\n",
        "    try:\n",
        "        obj.clearAttributes()\n",
        "        obj.reset()\n",
        "        while obj.incrementToken():\n",
        "            yield termAtt.toString() \n",
        "    finally:\n",
        "        obj.end()\n",
        "        obj.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vGUgatVYgl"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bLT6zmFVYgm",
        "outputId": "72ddde59-40e0-4ef6-aa40-f26190154ac3"
      },
      "source": [
        "from lucene import JArray_char, JArray\n",
        "\n",
        "from org.tartarus.snowball.ext import SpanishStemmer, EnglishStemmer\n",
        "\n",
        "def stem(stemmer, word):\n",
        "    # Add the word\n",
        "    stemmer.setCurrent(JArray_char(word), len(word))\n",
        "    # Fire stemming\n",
        "    stemmer.stem()\n",
        "    # Fetch the output (buffer & size)\n",
        "    result = stemmer.getCurrentBuffer()\n",
        "    l = stemmer.getCurrentBufferLength()\n",
        "    return ''.join(result)[0:l]    \n",
        "\n",
        "st = SpanishStemmer()\n",
        "for w in ('haciendo', 'lunes', 'vino', 'lápiz'):\n",
        "    print( w, '->', stem(st, w))\n",
        "\n",
        "st = EnglishStemmer()\n",
        "for w in ('making', 'Monday', 'came', 'pencil'):\n",
        "    print( w, '->', stem(st, w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "haciendo -> hac\n",
            "lunes -> lun\n",
            "vino -> vin\n",
            "lápiz -> lapiz\n",
            "making -> make\n",
            "Monday -> Monday\n",
            "came -> came\n",
            "pencil -> pencil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5A6HDkOVYgq"
      },
      "source": [
        "from java.io import StringReader\n",
        "\n",
        "def tokenize( tk, data ):\n",
        "    '''Send a string to a tokenizer and get back the token list'''\n",
        "    tk.setReader( StringReader(data) )\n",
        "    return list(fetch_terms(tk))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOpwfFJLVYgs"
      },
      "source": [
        "from org.apache.lucene.analysis.standard import StandardTokenizer\n",
        "from org.apache.lucene.analysis.core import LetterTokenizer\n",
        "from org.apache.lucene.analysis.ngram import NGramTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awaWVidxVYgu"
      },
      "source": [
        "# Tokenizer\n",
        "- StandardTokenizer : A grammar-based tokenizer constructed with JFlex. This class implements the Word Break rules from the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29.  \n",
        "- LetterTokenizer : A tokenizer that divides text at non-letters. That's to say, it defines tokens as maximal strings of adjacent letters, as defined by java.lang.Character.isLetter() predicate.  \n",
        "- NGramTokenizer : Tokenizes the input into n-grams of the given size(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLfN8_0XVYgu",
        "outputId": "b8790298-51cd-4a85-d42e-bca4885ba30a"
      },
      "source": [
        "tokenizers = (StandardTokenizer(), LetterTokenizer(), NGramTokenizer(4, 4))\n",
        "\n",
        "for n, t in enumerate(tokenizers):\n",
        "    print( \"\\n{} -----------\".format(n+1), str(t) )\n",
        "    for s in test_strings:\n",
        "        print( \"\\n\", tokenize(t,s) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1 ----------- StandardTokenizer@31ff43be term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1\n",
            "\n",
            " ['PyLucene', 'is', 'a', 'Python', 'extension', 'for', 'accessing', 'Java', 'Lucene']\n",
            "\n",
            " ['Its', 'goal', 'is', 'to', 'allow', 'you', 'to', 'use', \"Lucene's\", 'text', 'indexing', 'and', 'searching', 'capabilities', 'from', 'Python']\n",
            "\n",
            "2 ----------- LetterTokenizer@7b205dbd term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1\n",
            "\n",
            " ['PyLucene', 'is', 'a', 'Python', 'extension', 'for', 'accessing', 'Java', 'Lucene']\n",
            "\n",
            " ['Its', 'goal', 'is', 'to', 'allow', 'you', 'to', 'use', 'Lucene', 's', 'text', 'indexing', 'and', 'searching', 'capabilities', 'from', 'Python']\n",
            "\n",
            "3 ----------- NGramTokenizer@106cc338 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,termFrequency=1\n",
            "\n",
            " ['PyLu', 'yLuc', 'Luce', 'ucen', 'cene', 'ene ', 'ne i', 'e is', ' is ', 'is a', 's a ', ' a P', 'a Py', ' Pyt', 'Pyth', 'ytho', 'thon', 'hon ', 'on e', 'n ex', ' ext', 'exte', 'xten', 'tens', 'ensi', 'nsio', 'sion', 'ion ', 'on f', 'n fo', ' for', 'for ', 'or a', 'r ac', ' acc', 'acce', 'cces', 'cess', 'essi', 'ssin', 'sing', 'ing ', 'ng J', 'g Ja', ' Jav', 'Java', 'ava ', 'va L', 'a Lu', ' Luc', 'Luce', 'ucen', 'cene', 'ene.']\n",
            "\n",
            " ['Its ', 'ts g', 's go', ' goa', 'goal', 'oal ', 'al i', 'l is', ' is ', 'is t', 's to', ' to ', 'to a', 'o al', ' all', 'allo', 'llow', 'low ', 'ow y', 'w yo', ' you', 'you ', 'ou t', 'u to', ' to ', 'to u', 'o us', ' use', 'use ', 'se L', 'e Lu', ' Luc', 'Luce', 'ucen', 'cene', \"ene'\", \"ne's\", \"e's \", \"'s t\", 's te', ' tex', 'text', 'ext ', 'xt i', 't in', ' ind', 'inde', 'ndex', 'dexi', 'exin', 'xing', 'ing ', 'ng a', 'g an', ' and', 'and ', 'nd s', 'd se', ' sea', 'sear', 'earc', 'arch', 'rchi', 'chin', 'hing', 'ing ', 'ng c', 'g ca', ' cap', 'capa', 'apab', 'pabi', 'abil', 'bili', 'ilit', 'liti', 'itie', 'ties', 'ies ', 'es f', 's fr', ' fro', 'from', 'rom ', 'om P', 'm Py', ' Pyt', 'Pyth', 'ytho', 'thon', 'hon.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfRF3WvVYgw"
      },
      "source": [
        "# Analyzer\n",
        "- KeywordAnalyzer: \"Tokenizes\" the entire stream as a single token. This is useful for data like zip codes, ids, and some product names.  \n",
        "- SimpleAnalyzer: An Analyzer that filters LetterTokenizer with LowerCaseFilter \n",
        "- SpanishAnalyzer: built from an StandardTokenizer filtered with StandardFilter, LowerCaseFilter, StopFilter, SetKeywordMarkerFilter if a stem exclusion set is provided and SpanishLightStemFilter.  \n",
        "- ShingleAnalyzerWrapper: A ShingleAnalyzerWrapper wraps a ShingleFilter around another Analyzer. A shingle is another name for a token based n-gram.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4bSlgHZVYgw"
      },
      "source": [
        "from java.io import StringReader\n",
        "    \n",
        "def analyze(anal, data):\n",
        "    '''Send a string to an analizer and get back the analyzed term list'''\n",
        "    ts = anal.tokenStream( \"dummy\", StringReader(data) )\n",
        "    return list(fetch_terms(ts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpdixODQVYgy",
        "outputId": "5c3cd3d6-8777-4fe6-ac8e-199ee3fdcfdc"
      },
      "source": [
        "from org.apache.lucene.analysis.core import KeywordAnalyzer, SimpleAnalyzer\n",
        "from org.apache.lucene.analysis.standard import StandardAnalyzer\n",
        "from org.apache.lucene.analysis.es import SpanishAnalyzer\n",
        "from org.apache.lucene.analysis.shingle import ShingleAnalyzerWrapper\n",
        "\n",
        "analyzers = ( KeywordAnalyzer(),\n",
        "              SimpleAnalyzer(),\n",
        "              SpanishAnalyzer(),\n",
        "              ShingleAnalyzerWrapper( SimpleAnalyzer(), 2, 3 ),\n",
        "              ShingleAnalyzerWrapper( SpanishAnalyzer(), 2, 3 ),\n",
        "            )\n",
        "\n",
        "for n, a in enumerate(analyzers):\n",
        "    print( \"\\n {} ----------- {}\".format(n+1, a) )\n",
        "    for s in test_strings:\n",
        "        print( \"\\n\", analyze(a,s) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 1 ----------- org.apache.lucene.analysis.core.KeywordAnalyzer@2631f68c\n",
            "\n",
            " ['PyLucene is a Python extension for accessing Java Lucene.']\n",
            "\n",
            " [\"Its goal is to allow you to use Lucene's text indexing and searching capabilities from Python.\"]\n",
            "\n",
            " 2 ----------- org.apache.lucene.analysis.core.SimpleAnalyzer@19835e64\n",
            "\n",
            " ['pylucene', 'is', 'a', 'python', 'extension', 'for', 'accessing', 'java', 'lucene']\n",
            "\n",
            " ['its', 'goal', 'is', 'to', 'allow', 'you', 'to', 'use', 'lucene', 's', 'text', 'indexing', 'and', 'searching', 'capabilities', 'from', 'python']\n",
            "\n",
            " 3 ----------- org.apache.lucene.analysis.es.SpanishAnalyzer@a87f8ec\n",
            "\n",
            " ['pylucen', 'is', 'python', 'extension', 'for', 'accessing', 'java', 'lucen']\n",
            "\n",
            " ['its', 'goal', 'is', 'to', 'allow', 'you', 'to', 'use', \"lucene's\", 'text', 'indexing', 'and', 'searching', 'capabiliti', 'from', 'python']\n",
            "\n",
            " 4 ----------- org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper@22356acd\n",
            "\n",
            " ['pylucene', 'pylucene is', 'pylucene is a', 'is', 'is a', 'is a python', 'a', 'a python', 'a python extension', 'python', 'python extension', 'python extension for', 'extension', 'extension for', 'extension for accessing', 'for', 'for accessing', 'for accessing java', 'accessing', 'accessing java', 'accessing java lucene', 'java', 'java lucene', 'lucene']\n",
            "\n",
            " ['its', 'its goal', 'its goal is', 'goal', 'goal is', 'goal is to', 'is', 'is to', 'is to allow', 'to', 'to allow', 'to allow you', 'allow', 'allow you', 'allow you to', 'you', 'you to', 'you to use', 'to', 'to use', 'to use lucene', 'use', 'use lucene', 'use lucene s', 'lucene', 'lucene s', 'lucene s text', 's', 's text', 's text indexing', 'text', 'text indexing', 'text indexing and', 'indexing', 'indexing and', 'indexing and searching', 'and', 'and searching', 'and searching capabilities', 'searching', 'searching capabilities', 'searching capabilities from', 'capabilities', 'capabilities from', 'capabilities from python', 'from', 'from python', 'python']\n",
            "\n",
            " 5 ----------- org.apache.lucene.analysis.shingle.ShingleAnalyzerWrapper@34f22f9d\n",
            "\n",
            " ['pylucen', 'pylucen is', 'pylucen is _', 'is', 'is _', 'is _ python', '_ python', '_ python extension', 'python', 'python extension', 'python extension for', 'extension', 'extension for', 'extension for accessing', 'for', 'for accessing', 'for accessing java', 'accessing', 'accessing java', 'accessing java lucen', 'java', 'java lucen', 'lucen']\n",
            "\n",
            " ['its', 'its goal', 'its goal is', 'goal', 'goal is', 'goal is to', 'is', 'is to', 'is to allow', 'to', 'to allow', 'to allow you', 'allow', 'allow you', 'allow you to', 'you', 'you to', 'you to use', 'to', 'to use', \"to use lucene's\", 'use', \"use lucene's\", \"use lucene's text\", \"lucene's\", \"lucene's text\", \"lucene's text indexing\", 'text', 'text indexing', 'text indexing and', 'indexing', 'indexing and', 'indexing and searching', 'and', 'and searching', 'and searching capabiliti', 'searching', 'searching capabiliti', 'searching capabiliti from', 'capabiliti', 'capabiliti from', 'capabiliti from python', 'from', 'from python', 'python']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmysE5GlVYg0"
      },
      "source": [
        "### 참조: https://pythonhosted.org/lupyne/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZWKzZKYVYg1"
      },
      "source": [
        "# Lupyne is:\n",
        "- high-level Pythonic search engine library, built on PyLucene  \n",
        "- RESTful JSON search server, built on CherryPy  \n",
        "- simple Python client for interacting with the server  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQSUzfioVYg1"
      },
      "source": [
        "from lupyne import engine   # don't forget to call lucene.initVM\n",
        "\n",
        "indexer = engine.Indexer()                             # create an in-memory index (no filename supplied)\n",
        "indexer.set('name', stored=True)                     # create stored 'name' field\n",
        "indexer.set('text', engine.Field.Text)                  # create indexed 'text' field (the default)\n",
        "indexer.add(name='sample', text='hello world')  # add a document to the index\n",
        "indexer.commit()                                         # commit changes; document is now searchable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoSgNQlqVYg3"
      },
      "source": [
        "hits = indexer.search('text:hello')             # run search and return sequence of documents "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK7K5hUeVYg5",
        "outputId": "7553b37b-1641-4eb1-c26c-73707e187009"
      },
      "source": [
        "hits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lupyne.engine.documents.Hits at 0x21d682c2d48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqUfO-qXVYg7",
        "outputId": "65bea2c0-80a4-4b74-bb47-e33309c09b5c"
      },
      "source": [
        "len(hits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHPM9o_gVYg9",
        "outputId": "f89c271f-54b8-4607-e806-52285fc00dd7"
      },
      "source": [
        "hits.count "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-HWjyZdVYg_"
      },
      "source": [
        "# indexers\n",
        "Basic indexing and searching example adapted from http://lucene.apache.org/core/4_10_1/core/index.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjv1qtXEVYg_",
        "outputId": "ab3d1b0a-0b51-4afa-c83f-01132656da98"
      },
      "source": [
        "import lucene\n",
        "from org.apache.lucene import analysis, document, index, queryparser, search, store, util\n",
        "from lupyne import engine\n",
        "lucene.initVM()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<jcc.JCCEnv at 0x21d682b45d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYMEsUf2VYhB"
      },
      "source": [
        "# # # lucene # # #\n",
        "\n",
        "analyzer = analysis.standard.StandardAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCxUY55mVYhC"
      },
      "source": [
        "# Store the index in memory:\n",
        "directory = store.RAMDirectory()\n",
        "\n",
        "# To store an index on disk, use this instead:\n",
        "# Directory directory = FSDirectory.open(File(\"/tmp/testindex\"))\n",
        "config = index.IndexWriterConfig( analyzer)\n",
        "iwriter = index.IndexWriter(directory, config)\n",
        "doc = document.Document()\n",
        "text = \"This is the text to be indexed.\"\n",
        "doc.add(document.Field(\"fieldname\", text, document.TextField.TYPE_STORED))\n",
        "iwriter.addDocument(doc)\n",
        "iwriter.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4a8uEt5VYhE"
      },
      "source": [
        "# Now search the index:\n",
        "# ireader = index.IndexReader.open(directory)\n",
        "ireader = index.DirectoryReader.open(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKV49mX4VYhG"
      },
      "source": [
        "isearcher = search.IndexSearcher(ireader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TsycCaVYhI"
      },
      "source": [
        "# Parse a simple query that searches for \"text\":\n",
        "# parser = queryparser.classic.QueryParser(util.Version.LUCENE_CURRENT, \"fieldname\", analyzer)\n",
        "parser = queryparser.classic.QueryParser(\"fieldname\", analyzer)\n",
        "query = parser.parse(\"text\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39NrDV6pVYhK"
      },
      "source": [
        "hits = isearcher.search(query, 1000).scoreDocs\n",
        "assert len(hits) == 1\n",
        "\n",
        "# Iterate through the results:\n",
        "for hit in hits:\n",
        "    hitDoc = isearcher.doc(hit.doc)\n",
        "    assert hitDoc['fieldname'] == text\n",
        "ireader.close()\n",
        "directory.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqVtLblVVYhM"
      },
      "source": [
        "# queries\n",
        "- Convenient Query creation.  \n",
        "- Operator overloading is used for combining boolean clauses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHKxFp2ZVYhN",
        "outputId": "e2621d53-6eaf-43b1-e8cf-677672592bf8"
      },
      "source": [
        "import lucene\n",
        "from org.apache.lucene import index, search\n",
        "from org.apache.lucene.search import spans\n",
        "from lupyne.engine import Query\n",
        "lucene.initVM()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<jcc.JCCEnv at 0x21d68568810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QciEocnSVYhP",
        "outputId": "6762d95d-aadc-4673-f7b9-9699fc838de6"
      },
      "source": [
        "# # # lucene # # #\n",
        "\n",
        "q1 = search.TermQuery(index.Term('text', 'lucene'))\n",
        "q2 = search.PhraseQuery()\n",
        "q2.add(index.Term('text', 'search'))\n",
        "q2.add(index.Term('text', 'engine'))\n",
        "q3 = search.BooleanQuery()\n",
        "q3.add(q1, search.BooleanClause.Occur.MUST)\n",
        "q3.add(q2, search.BooleanClause.Occur.MUST)\n",
        "assert str(q3) == '+text:lucene +text:\"search engine\"'\n",
        "\n",
        "q1 = spans.SpanTermQuery(index.Term('text', 'hello'))\n",
        "q2 = spans.SpanTermQuery(index.Term('text', 'world'))\n",
        "q3 = spans.SpanPositionRangeQuery(q1, 0, 10)\n",
        "q4 = spans.SpanNearQuery([q1, q2], 0, True)\n",
        "q5 = spans.SpanNotQuery(q3, q4)\n",
        "assert str(q5) == 'spanNot(spanPosRange(text:hello, 0, 10), spanNear([text:hello, text:world], 0, true), 0, 0)'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgsError",
          "evalue": "(<class 'org.apache.lucene.search.PhraseQuery'>, '__init__', ())",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgsError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-30-eae8ef76f51a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTermQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTerm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lucene'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mq2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhraseQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTerm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'search'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTerm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'engine'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mInvalidArgsError\u001b[0m: (<class 'org.apache.lucene.search.PhraseQuery'>, '__init__', ())"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxzW8tV8VYhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MomJG2aJVYhS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}